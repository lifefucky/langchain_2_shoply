import argparse
import json

import logging
import os
import re
import sys
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Union, Optional

from dotenv import load_dotenv
from langchain.callbacks import get_openai_callback

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, \
    ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

from dataclasses import dataclass
from pydantic import BaseModel, Field, conlist
import yaml


#actions –ø—É—Å—Ç–æ–π

class FAQResponse(BaseModel):
    answer: str = Field(..., description="–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    tone: str = Field(..., description="–ö–æ–Ω—Ç—Ä–æ–ª—å —Ç–æ–Ω–∞: '–¥–∞' –µ—Å–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –±—Ä–µ–Ω–¥—É, –∏–Ω–∞—á–µ '–Ω–µ—Ç' —Å –∫—Ä–∞—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–Ω–µ—Ç, —Å–ª–∏—à–∫–æ–º —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π')")
    actions: conlist(str, max_length=3) = Field(
        default_factory=list,
        description="–°–ø–∏—Å–æ–∫ –∏–∑ 1-3 —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞"
    )

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è LLM –∏ embeddings"""
    api_key: str
    base_url: str = ""
    embedding_model: str = "text-embedding-3-small"
    llm_model: str = "gpt-4o-mini"
    temperature: float = 0.3
    context_length: int = 3
    brand_name: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Å–ª–æ–≤–∞—Ä—å —Å –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π API –∫–ª—é—á–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –°–ª–æ–≤–∞—Ä—å —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "embedding_model": self.embedding_model,
            "llm_model": self.llm_model,
            "temperature": self.temperature,
            "context_length": self.context_length
        }


def setup_api_config() -> ModelConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è -> .env —Ñ–∞–π–ª -> —Ä—É—á–Ω–æ–π –≤–≤–æ–¥"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    load_dotenv()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("LLM_API_BASE_URL")
    embedding_model = os.getenv("EMBEDDING_MODEL")
    llm_model = os.getenv("LLM_MODEL")
    temperature = float(os.getenv("LLM_TEMPERATURE"))
    context_length = int(os.getenv("CONTEXT_LENGTH"))
    brand_name = os.getenv("BRAND_NAME")

    return ModelConfig(
        api_key=api_key,
        base_url=base_url,
        embedding_model=embedding_model,
        llm_model=llm_model,
        temperature=temperature,
        context_length=context_length,
        brand_name=brand_name
    )


class Consultant:
    def __init__(self, model_config: ModelConfig):
        self.model_config = model_config

        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(script_dir, "data")
        with open(os.path.join(self.data_dir, "faq.json"), mode='r', encoding='utf-8') as file:
            self.faq = json.load(file)
        with open(os.path.join(self.data_dir, "orders.json"), mode='r', encoding="utf-8") as file:
            self.orders = json.load(file)
        with open(os.path.join(script_dir, 'model_configs.yaml'), mode='r', encoding='utf-8') as file:
            self._prompt_config = yaml.safe_load(file)

        os.makedirs("logs", exist_ok=True)
        now: str = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.logger = self.setup_logger(f"logs/session_{now}.jsonl")

        self.conversation_history: list[dict[str, str]] = []
        self.context_length = self.model_config.context_length

        self.few_shot_examples = self._load_few_shot_examples()
        self.example_selector = self._init_example_selector()

    @staticmethod
    def setup_logger(log_file):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        handler = logging.FileHandler(log_file, encoding="utf-8")
        handler.setFormatter(logging.Formatter("%(message)s"))
        logger.addHandler(handler)

        return logger

    def format_conversation_history(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ —Å—Ç—Ä–æ–∫—É"""
        return "\n".join(
            [f"{msg['role'].upper()}: {msg['content']}"
             for msg in self.conversation_history[-self.context_length:]]
        )

    def add_to_history(self, role: str, content: str):
        self.conversation_history.append({"role": role, "content": content})

    def prepare_text_faq(self) -> List[str]:
        return [f"–í–æ–ø—Ä–æ—Å:'{qa['q']}'\n–û—Ç–≤–µ—Ç:'{qa['a']}'" for qa in self.faq]

    def create_vector_store(self) -> FAISS:
        texts = self.prepare_text_faq()

        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ embedding –º–æ–¥–µ–ª–∏
            embedding_kwargs = {
                "api_key": self.model_config.api_key,
                "model": self.model_config.embedding_model
            }

            if self.model_config.base_url:
                embedding_kwargs["base_url"] = self.model_config.base_url

            embeddings = OpenAIEmbeddings(**embedding_kwargs)

            documents = [Document(page_content=text) for text in texts]

            text_splitter = CharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separator="\n"
            )
            docs = text_splitter.split_documents(documents)

            vector_store = FAISS.from_documents(docs, embeddings)
            self.add_log(event='vector_store_created', document_count=len(docs), chunks_count=len(docs))
            return vector_store

        except Exception as e:
            details = {
                "documents_count": len(texts) if 'texts' in locals() else 0
            }
            self.add_log(type='error', message=str(e), details=details, event_type='vector_store_creation')
            raise

    def system_prompt(self):
        pass

    def user_prompt(self):
        pass

    def _load_few_shot_examples(self) -> List[Dict[str, Any]]:
        examples_path = os.path.join(self.data_dir, 'few_shots.jsonl')
        if not os.path.exists(examples_path) or os.path.getsize(examples_path) == 0:
            return []

        examples = []
        with open(examples_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        output_str = json.dumps({
                            "answer": data["output"]["answer"],
                            "tone": data["output"]["tone"],
                            "actions": data["output"]["actions"]
                        }, ensure_ascii=False)
                        examples.append({
                            "input": data["input"],
                            "context": data.get("context", ""),
                            "output": data["output"]  # –ï–¥–∏–Ω–æ–µ –ø–æ–ª–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
                        })

                        '''examples.append({
                            "input": data["input"],
                            "context": data.get("context", ""),
                            "answer": data["output"]["answer"],
                            "tone": data["output"]["tone"],
                            "actions": json.dumps(data["output"]["actions"], ensure_ascii=False)
                        })'''
                    except Exception as e:
                        self.add_log(
                            type='error',
                            message=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ few-shot –ø—Ä–∏–º–µ—Ä–∞: {str(e)}",
                            event='few_shot_error'
                        )
        return examples

    def _init_example_selector(self) -> Optional[SemanticSimilarityExampleSelector]:
        if not self.few_shot_examples:
            return None

        try:
            embedding_kwargs = {
                "api_key": self.model_config.api_key,
                "model": self.model_config.embedding_model
            }
            if self.model_config.base_url:
                embedding_kwargs["base_url"] = self.model_config.base_url

            embeddings = OpenAIEmbeddings(**embedding_kwargs)

            example_texts = [
                f"–í–æ–ø—Ä–æ—Å: {ex['input']}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ex['context']}"
                for ex in self.few_shot_examples
            ]

            metadatas = []
            for ex in self.few_shot_examples:
                metadatas.append({
                    "input": ex["input"],
                    "output": json.dumps(ex["output"], ensure_ascii=False),
                    "context": ex.get("context", "")
                })

            vectorstore = FAISS.from_texts(
                example_texts,
                embeddings,
                metadatas=metadatas
            )

            self.add_log(
                event='few_shot_selector_initialized',
                example_count=len(self.few_shot_examples),
                model=self.model_config.embedding_model
            )

            return SemanticSimilarityExampleSelector(
                vectorstore=vectorstore,
                k=min(2, len(self.few_shot_examples)),
                input_keys=["input"],
            )

        except Exception as e:
            self.add_log(
                type='error',
                message=f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ few-shot —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {str(e)}",
                event='few_shot_init_error'
            )
            return None

    def retrieval_chain(self, model: ChatOpenAI, vector_store: FAISS):
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )

        chain_config = self._prompt_config["prompts"]["support_answer"]
        current_version = chain_config["current_version"]
        version_config = chain_config["versions"][current_version]

        messages = []
        if system_template:= version_config.get('system'):
            system_prompt = SystemMessagePromptTemplate(
                prompt=PromptTemplate(
                    template=system_template
                )
            )
            #self.add_log(message=system_prompt.prompt)
            messages.append(system_prompt)

        if self.example_selector:
            example_prompt = ChatPromptTemplate.from_messages([
                ("human", "{input}"),
                ("ai", "{output}")
            ])

            few_shot_prompt = FewShotChatMessagePromptTemplate(
                example_selector=self.example_selector,
                example_prompt=example_prompt,
                input_variables=["input"]
            )
            #self.add_log(message=few_shot_prompt.prompt)
            messages.append(few_shot_prompt)

        # Human –ø—Ä–æ–º–ø—Ç —Å –≤–æ–ø—Ä–æ—Å–æ–º
        user_template = version_config['user']
        user_prompt = HumanMessagePromptTemplate(
            prompt=PromptTemplate(
                template=user_template
            )
        )
        #self.add_log(event='human_message_prompt_t', message=user_prompt.prompt)
        messages.append(user_prompt)

        input_vars = version_config["input_variables"].copy()

        self.add_log(event='awaiting_variables', message=input_vars)
        qa_prompt = ChatPromptTemplate(
            messages=messages,
            input_variables=input_vars
        )

        document_chain = create_stuff_documents_chain(
            model.with_structured_output(FAQResponse, method="function_calling", include_raw=False),
            qa_prompt
        )
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        try:
            input_schema = retrieval_chain.get_input_schema()
            input_keys = list(input_schema.model_fields.keys())
            self.add_log(event="chain_created", input_keys=input_keys)
        except Exception as e:
            self.add_log(type='warning', message=f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –∫–ª—é—á–∏ —Ü–µ–ø–æ—á–∫–∏: {str(e)}")
        return retrieval_chain

    def faq_processor(self, query: str, retrieval_chain):
        try:
            with get_openai_callback() as cb:
                full_response = retrieval_chain.invoke({
                    "brand_name": self.model_config.brand_name,
                    "input": query,
                    "history": self.format_conversation_history()
                })

                response: FAQResponse = full_response["answer"]

                if not response.actions or len(response.actions) == 0:
                    self.add_log(
                        type='warning',
                        query=query,
                        message="actions –±—ã–ª –ø—É—Å—Ç, –ø—Ä–∏–º–µ–Ω–µ–Ω fallback",
                        another_actions=json.dumps(response.model_dump_json(), ensure_ascii=False)
                    )
                    response.actions = [
                        "–ó–∞–¥–∞–π—Ç–µ —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å",
                        "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏"
                    ]


                serializable_response = {
                    "answer": response.answer,
                    "tone": response.tone,
                    "actions": response.actions,
                    "usage": {"total_tokens": cb.total_tokens, "prompt_tokens": cb.prompt_tokens,
                              "completion_tokens": cb.completion_tokens}
                }

            self.add_log(query=query, message=serializable_response)
            return response
        except Exception as e:
            error_context = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "chain_input_keys": retrieval_chain.input_keys if hasattr(retrieval_chain, 'input_keys') else None,
                "query": query,
                "history": self.format_conversation_history()
            }
            self.add_log(type='error', message="Chain execution failed", **error_context)
            print("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")

            return FAQResponse(
                answer="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
                tone="–æ—à–∏–±–∫–∞",
                actions=[]
            )


    def orders_processor(self, query: str):
        match = re.fullmatch(r'/order\s+(\d+)', query.strip())
        if not match:
            response = '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /order <–Ω–æ–º–µ—Ä>'
            self.add_log(type='error', query=query, message=response, event='order_error', event_type='invalid_format')
            return response

        order_id = match.group(1)
        if order := self.orders.get(order_id):
            response = f'–ó–∞–∫–∞–∑ #{match.group(1)}: {format_order_details(order)}'
            self.add_log(query=query, message=response)
            return response
        else:
            response = '–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.'
            self.add_log(type='error', query=query, message=response, event='order_error', event_type='not_found')
            return response

    def add_log(self, type: str = "info", query: str = None, message: Union[str, dict] = None, **kwargs):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "message": message,
            **kwargs
        }
        if type == 'error':
            self.logger.error(json.dumps(log_entry, ensure_ascii=False))
        else:
            self.logger.info(json.dumps(log_entry, ensure_ascii=False))


def format_order_details(info: dict) -> str:
    status = info.get("status")
    if status == "in_transit":
        eta = info.get("eta_days", 0)
        carrier = info.get("carrier", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
        detail = f"–ó–∞–∫–∞–∑ –≤ –ø—É—Ç–∏. –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ —á–µ—Ä–µ–∑ {eta} –¥–Ω. –ü–µ—Ä–µ–≤–æ–∑—á–∏–∫: {carrier}."
    elif status == "delivered":
        delivered_at = info.get("delivered_at", "–Ω–µ —É–∫–∞–∑–∞–Ω–∞")
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –º–æ–∂–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç—É –∫—Ä–∞—Å–∏–≤–æ
            date_obj = datetime.strptime(delivered_at, "%Y-%m-%d")
            delivered_at = date_obj.strftime("%d.%m.%Y")
        except ValueError:
            pass  # –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
        detail = f"–ó–∞–∫–∞–∑ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω {delivered_at}."
    elif status == "processing":
        note = info.get("note", "–ë–µ–∑ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π")
        detail = f"–ó–∞–∫–∞–∑ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ. {note}"
    else:
        detail = f"–°—Ç–∞—Ç—É—Å –∑–∞–∫–∞–∑–∞: {status}." if status else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–∫–∞–∑–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."

    return detail


def main():
    parser = argparse.ArgumentParser(description="Consultant Bot")
    parser.add_argument('--url', type=str, help='Base URL for LLM API')
    parser.add_argument('--model', type=str, help='LLM model name')
    parser.add_argument('--api-key', type=str, help='API key for authentication')
    args = parser.parse_args()

    try:
        model_config = setup_api_config()
        if args.api_key:
            model_config.api_key = args.api_key
        if args.url:
            model_config.base_url = args.url
        if args.model:
            model_config.llm_model = args.model

        bot = Consultant(model_config=model_config)
        config = bot.model_config
        bot.add_log(event="config_loaded", config=config.to_dict())

        llm_kwargs = {
            "api_key": config.api_key,
            "temperature": config.temperature,
            "model_name": config.llm_model,
            "openai_api_base": config.base_url}
        model = ChatOpenAI(**llm_kwargs)

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        vector_store = bot.create_vector_store()
        retrieval_chain = bot.retrieval_chain(model=model, vector_store=vector_store)

        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\n" + "=" * 50)
        print("–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

        while True:
            query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            bot.add_to_history("user", query)
            if query.lower() in ['exit', 'quit', '–≤—ã–π—Ç–∏']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üê±")
                bot.add_log(message="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–ª –≤—ã—Ö–æ–¥.")
                break

            if not query:
                continue

            if query.startswith("/order"):
                response = bot.orders_processor(query=query)
                print(response)
                bot.add_to_history("assistant", response)
                continue

            response = bot.faq_processor(query=query, retrieval_chain=retrieval_chain)
            print(response.answer)
            bot.add_to_history("assistant", response.answer)

    except Exception as e:
        error_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": "critical_error",
            "error": str(e),
            "traceback": traceback.format_exc() if 'traceback' in sys.modules else None
        }
        logging.getLogger(__name__).error(json.dumps(error_entry, ensure_ascii=False))
        print("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è. –î–µ—Ç–∞–ª–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –ª–æ–≥.")
        exit(1)


if __name__ == "__main__":
    main()